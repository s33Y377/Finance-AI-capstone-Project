# -*- coding: utf-8 -*-
"""Finance.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MQnfN0oeDLpv5u_MbkcKurRXob5lKxa2

DESCRIPTION

Problem Statement

Amazon is an online shopping website that now caters to millions of people everywhere. Over 34,000 consumer reviews for Amazon brand products like Kindle, Fire TV Stick and more are provided. 
The dataset has attributes like brand, categories, primary categories, reviews.title, reviews.text, and the sentiment. Sentiment is a categorical variable with three levels "Positive", "Negative“, and "Neutral". For a given unseen data, the sentiment needs to be predicted.
You are required to predict Sentiment or Satisfaction of a purchase based on multiple features and review text.
Dataset Snapshot
DESCRIPTION

Problem Statement

Finance Industry is the biggest consumer of Data Scientists. It faces constant attack by fraudsters, who try to trick the system. Correctly identifying fraudulent transactions is often compared with finding needle in a haystack because of the low event rate. 
It is important that credit card companies are able to recognize fraudulent credit card transactions so that the customers are not charged for items that they did not purchase.
You are required to try various techniques such as supervised models with oversampling, unsupervised anomaly detection, and heuristics to get good accuracy at fraud detection.
Dataset Snapshot

The datasets contain transactions made by credit cards in September 2013 by European cardholders. This dataset represents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

 



 

It contains only numerical input variables which are the result of a PCA transformation. 
Features V1, V2, ... V28 are the principal components obtained with PCA. 
The only features which have not been transformed with PCA are 'Time' and 'Amount'

 

Project Task: Week 1

Exploratory Data Analysis (EDA):

1.    Perform an EDA on the Dataset.
       a)    Check all the latent features and parameters with their mean and standard deviation. Value are close to 0 centered (mean)
              with unit standard deviation
       b)    Find if there is any connection between Time, Amount, and the transaction being fraudulent.
2.    Check the class count for each class. It’s a class Imbalance problem.
3.    Use techniques like undersampling or oversampling before running Naïve Bayes, Logistic Regression or SVM.
       a.    Oversampling or undersampling can be used to tackle the class imbalance problem
       b.    Oversampling increases the prior probability of imbalanced class and in case of other classifiers, error gets multiplied as the 
              low-proportionate class is mimicked multiple times.
4     Following are the matrices for evaluating the model performance: Precision, Recall, F1-Score, AUC-ROC curve. Use F1-Score as
       the evaluation criteria for this project.

Project Task: Week 2

Modeling Techniques:

Try out models like Naive Bayes, Logistic Regression or SVM. Find out which one performs the best
Use different Tree-based classifiers like Random Forest and XGBoost. 
       a.    Remember Tree-based classifiers work on two ideologies: Bagging or Boosting
       b.    Tree-based classifiers have fine-tuning parameters which takes care of the imbalanced class. Random-Forest and XGBboost.
Compare the results of 1 with 2 and check if there is any incremental gain.

Project Task: Week 3

Applying ANN:

Use ANN (Artificial Neural Network) to identify fradulent and non-fradulent.
       a)    Fine-tune number of layers
       b)    Number of Neurons in each layers
       c)    Experiment in batch-size
       d)    Experiment with number of epochs. Check the observations in loss and accuracy
       e)    Play with different Learning Rate variants of Gradient Descent like Adam, SGD, RMS-prop
       f)    Find out which activation performs best for this use case and why?
       g)    Check Confusion Matrix, Precision, Recall and F1-Score
2.    Try out Dropout for ANN. How is it performed? Compare model performance with the traditional ML based prediction models from
       above. 
3.    Find the best setting of neural net that can be best classified as fraudulent and non-fraudulent transactions. Use
       techniques like Grid Search, Cross-Validation and Random search.

Anomaly Detection:

4.     Implement anomaly detection algorithms.
        a)    Assume that the data is coming from a single or a combination of multivariate Gaussian
        b)    Formalize a scoring criterion, which gives a scoring probability for the given data point whether it belongs to the
              multivariate Gaussian or Normal Distribution fitted in a)
Project Task: Week 4

Inference and Observations:

Visualize the scores for Fraudulent and Non-Fraudulent transactions.
Find out the threshold value for marking or reporting a transaction as fraudulent in your anomaly detection system.
Can this score be used as an engineered feature in the models developed previously? Are there any incremental gains in F1-Score? Why or Why not?
Be as creative as possible in finding other interesting insights.

Project Task: Week 1

Class Imbalance Problem:

1. Perform an EDA on the dataset.

       a)  See what a positive, negative, and neutral review looks like

       b)  Check the class count for each class. It’s a class imbalance problem.

2. Convert the reviews in Tf-Idf score.

3. Run multinomial Naive Bayes classifier. Everything will be classified as positive because of the class imbalance.

Project Task: Week 2

Tackling Class Imbalance Problem:

Oversampling or undersampling can be used to tackle the class imbalance problem. 
In case of class imbalance criteria, use the following metrices for evaluating model performance: precision, recall, F1-score, AUC-ROC curve. Use F1-Score as the evaluation criteria for this      project.
Use Tree-based classifiers like Random Forest and XGBoost.
       Note: Tree-based classifiers work on two ideologies namely, Bagging or Boosting and have fine-tuning parameter which takes care of the imbalanced class.

Project Task: Week 3

Model Selection:

Apply multi-class SVM’s and neural nets.
Use possible ensemble techniques like: XGboost + oversampled_multinomial_NB.
Assign a score to the sentence sentiment (engineer a feature called sentiment score). Use this engineered feature in the model and check for improvements. Draw insights on the same.
Project Task: Week 4

Applying LSTM:

Use LSTM for the previous problem (use parameters of LSTM like top-word, embedding-length, Dropout, epochs, number of layers, etc.)
       Hint: Another variation of LSTM, GRU (Gated Recurrent Units) can be tried as well.

      2. Compare the accuracy of neural nets with traditional ML based algorithms.

      3. Find the best setting of LSTM (Neural Net) and GRU that can best classify the reviews as positive, negative, and neutral. 

       Hint: Use techniques like Grid Search, Cross-Validation and Random Search

Optional Tasks: Week 4

Topic Modeling:

   1. Cluster similar reviews.
       Note: Some reviews may talk about the device as a gift-option. Other reviews may be about product looks and some may
          highlight about its battery and performance. Try naming the clusters.
   2. Perform Topic Modeling
       Hint: Use scikit-learn provided Latent Dirchlette Allocation (LDA) and Non-Negative Matrix Factorization (NMF).
"""

import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np
import time
from datetime import datetime
from joblib import Parallel, delayed
import os

import tensorflow as tf
from tensorflow.keras.layers import Dense,Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
import tensorboard
from keras.utils.vis_utils import plot_model
from ann_visualizer.visualize import ann_viz

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc, f1_score, precision_score, recall_score
from sklearn.svm import SVC, LinearSVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier

from imblearn.ensemble import BalancedRandomForestClassifier
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import MinMaxScaler, Normalizer, MaxAbsScaler, StandardScaler, StandardScaler, RobustScaler
from sklearn.model_selection import GridSearchCV, cross_val_score, GridSearchCV, train_test_split, cross_validate, KFold
from sklearn.pipeline import Pipeline
import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn.naive_bayes import GaussianNB, MultinomialNB
plt.style.use('ggplot')
sns.set()

from dask.distributed import Client

client = Client("tcp://127.0.0.1:64573")
client

tf.__version__

pd.set_option('precision', 3)

df_train = pd.read_csv('Financial/train_data.csv')
df_train_hidden = pd.read_csv('Financial/test_data_hidden.csv')
df_test = pd.read_csv('Financial/test_data.csv')
frame = [df_train,df_train_hidden]
df = pd.concat(frame)

"""# EDA"""

df.shape

df.head()

"""## Perform an EDA on the Dataset.
   * ### Check all the latent features and parameters with their mean and standard deviation. Value are close to 0 centered (mean) with unit standard deviation
"""

df.iloc[:, [x for x in range(1,29)]].describe()

df.info()

df.Class.value_counts()

# df.Class.plot.bar()
sns.distplot(df.Class)

"""* ### Find if there is any connection between Time, Amount, and the transaction being fraudulent.

#### Time
"""

#visualizations of time and amount
plt.figure(figsize=(10,8))
plt.title('Distribution of Time Feature')
sns.distplot(df.Time)

# Let's convert the time from seconds to hours to ease the interpretation.
df.loc[:,'Time'] = df.Time / 3600
df['Time'].max() / 24

plt.figure(figsize=(12,4), dpi=80)
sns.distplot(df['Time'], bins=48, kde=False)
plt.xlim([0,48])
plt.xticks(np.arange(0,54,6))
plt.xlabel('Time After First Transaction (hr)')
plt.ylabel('Count')
plt.title('Transaction Times')

df['Time'].describe()

"""#### Amount"""

df['Amount'].describe()

plt.figure(figsize=(12,4), dpi=80)
sns.distplot(df['Amount'], bins=300, kde=False)
plt.ylabel('Count')
plt.title('Transaction Amounts')

plt.figure(figsize=(12,4), dpi=80)
sns.boxplot(df['Amount'])
plt.title('Transaction Amounts')

df['Amount'].skew()

"""#### Time vs Amount"""

sns.jointplot(df['Time'].apply(lambda x: x % 24), df['Amount'], kind='hex', stat_func=None, size=12, xlim=(0,24), ylim=(-7.5,14)).set_axis_labels('Time of Day (hr)','Transformed Amount')

df.hist(figsize = (20, 20))
plt.show()

# 

# std_scaler = StandardScaler()
# rob_scaler = RobustScaler()

# df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))
# df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))

# df.drop(['Time','Amount'], axis=1, inplace=True)
# df.head()

# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.

# Lets shuffle the data before creating the subsamples

df = df.sample(frac=1)

# amount of fraud classes 492 rows.
fraud_df = df.loc[df['Class'] == 1]
non_fraud_df = df.loc[df['Class'] == 0][:394]

normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

# Shuffle dataframe rows
new_df = normal_distributed_df.sample(frac=1, random_state=42)

new_df.head()

new_df.shape

sns.countplot('Class', data=new_df)

#visualizing the features w high negative correlation
f, axes = plt.subplots(nrows=2, ncols=4, figsize=(26,16))

f.suptitle('Features With High Negative Correlation', size=35)
sns.boxplot(x="Class", y="V3", data=df, ax=axes[0,0])
sns.boxplot(x="Class", y="V9", data=df, ax=axes[0,1])
sns.boxplot(x="Class", y="V10", data=df, ax=axes[0,2])
sns.boxplot(x="Class", y="V12", data=df, ax=axes[0,3])
sns.boxplot(x="Class", y="V14", data=df, ax=axes[1,0])
sns.boxplot(x="Class", y="V16", data=df, ax=axes[1,1])
sns.boxplot(x="Class", y="V17", data=df, ax=axes[1,2])
f.delaxes(axes[1,3])

f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))

# Entire DataFrame
corr = df.corr()
sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)
ax1.set_title("Imbalanced Correlation Matrix \n (don't use for reference)", fontsize=14)


sub_sample_corr = new_df.corr()
sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)
ax2.set_title('SubSample Correlation Matrix \n (use for reference)', fontsize=14)
plt.show()

f, axes = plt.subplots(ncols=5, figsize=(25,4))

# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)
sns.boxplot(x="Class", y="V17", data=new_df, ax=axes[0])
axes[0].set_title('V17 vs Class Negative Correlation')

sns.boxplot(x="Class", y="V14", data=new_df, ax=axes[1])
axes[1].set_title('V14 vs Class Negative Correlation')


sns.boxplot(x="Class", y="V12", data=new_df, ax=axes[2])
axes[2].set_title('V12 vs Class Negative Correlation')


sns.boxplot(x="Class", y="V10", data=new_df, ax=axes[3])
axes[3].set_title('V10 vs Class Negative Correlation')

sns.boxplot(x="Class", y="V28", data=new_df, ax=axes[4])
axes[3].set_title('V10 vs Class Negative Correlation')

plt.show()

f, axes = plt.subplots(ncols=4, figsize=(20,4))

# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)
sns.boxplot(x="Class", y="V11", data=new_df, ax=axes[0])
axes[0].set_title('V11 vs Class Positive Correlation')

sns.boxplot(x="Class", y="V4", data=new_df, ax=axes[1])
axes[1].set_title('V4 vs Class Positive Correlation')


sns.boxplot(x="Class", y="V2", data=new_df, ax=axes[2])
axes[2].set_title('V2 vs Class Positive Correlation')


sns.boxplot(x="Class", y="V19", data=new_df, ax=axes[3])
axes[3].set_title('V19 vs Class Positive Correlation')

plt.show()

from scipy.stats import norm

f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))

v14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values
sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')
ax1.set_title('V14 Distribution \n (Fraud Transactions)', fontsize=14)

v12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values
sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')
ax2.set_title('V12 Distribution \n (Fraud Transactions)', fontsize=14)


v10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values
sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')
ax3.set_title('V10 Distribution \n (Fraud Transactions)', fontsize=14)

plt.show()

std_scaler = StandardScaler()
rob_scaler = RobustScaler()

df_train['scaled_amount'] = rob_scaler.fit_transform(df_train['Amount'].values.reshape(-1,1))
df_train['scaled_time'] = rob_scaler.fit_transform(df_train['Time'].values.reshape(-1,1))

df_train.drop(['Time','Amount'], axis=1, inplace=True)
df_train.head()

y = df_train.pop('Class')
x = df_train

df_test.head()

x = x.drop(['scaled_amount','scaled_time'],axis=1)
df_test = df_test.drop(['Time','Amount'],axis=1)

x

x.shape

y.shape

clf = RandomForestClassifier(max_depth=2, random_state=0)
clf.fit(x, y)
clf.feature_importances_

xb = xgb.XGBRFRegressor()
xb.fit(x, y)
xb.feature_importances_

def roc_curve_plots(y_test,y_predict_wrf,X_test,model):
    print(classification_report(y_test,y_predict_wrf),"\n")
    neigh_prob_linear=model.predict_proba(X_test)
    neigh_prob_linear1=neigh_prob_linear[:,1]
    fpr,tpr,thresh=roc_curve(y_test,neigh_prob_linear1)
    roc_auc_neigh=auc(fpr,tpr)

    plt.figure(dpi=80)
    plt.title("ROC Curve")
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.plot(fpr,tpr,'b',label='AUC Score = %0.2f'%roc_auc_neigh)
    plt.plot(fpr,fpr,'r--',color='red')
    plt.legend()

def model_accuracies(model,x_feature,y_label,X_test,df_test_len):
    # X_train, X_test, y_train, y_test = train_test_split(x_feature, y_label, test_size=0.33, random_state=42)
    print(x_feature.shape,y_label.shape,X_test.shape)
    y_pred = model.fit(x_feature, y_label).predict(X_test)
    print("df_test_len", df_test_len)
    cm_wrf = confusion_matrix(y_pred,y_label.iloc[:df_test_len])
    print("f1_score       : ", f1_score(y_label.iloc[:df_test_len], y_pred, average="macro"))
    print("precision_score: ", precision_score(y_label.iloc[:df_test_len], y_pred, average="macro"))
    print("recall_score   : ", recall_score(y_label.iloc[:df_test_len], y_pred, average="macro"))
    print("\nAccuracy Score :",accuracy_score(y_pred,y_label.iloc[:df_test_len]))
    roc_curve_plots(y_label.iloc[:df_test_len],y_pred,X_test,model)

"""## Use techniques like undersampling or oversampling before running Naïve Bayes, Logistic Regression or SVM.
   * ### Oversampling or undersampling can be used to tackle the class imbalance problem
   * ### Oversampling increases the prior probability of imbalanced class and in case of other classifiers, error gets multiplied as the low-proportionate class is mimicked multiple times.
"""

ros = RandomOverSampler(random_state=42)
X_ros, y_ros = ros.fit_resample(x, y)

print(X_ros.shape, y_ros.shape)

rus = RandomUnderSampler(random_state=42)
X_rus, y_rus = rus.fit_resample(x, y)

print(X_rus.shape, y_rus.shape)

gnb = GaussianNB()
df_test_len = df_test.shape[0]
# print(df_test.sample(n=df_test_len))
model_accuracies(model = gnb, x_feature=X_ros,y_label= y_ros, X_test=df_test, df_test_len=df_test_len)

lr = LogisticRegression(class_weight ='balanced')
model_accuracies(model = lr, x_feature=X_ros,y_label= y_ros,X_test=df_test,df_test_len=df_test.shape[0])

gnb = GaussianNB()
model_accuracies(model = gnb, x_feature=X_rus,y_label= y_rus, X_test=df_test.iloc[:788],df_test_len = 788)

lr = LogisticRegression(class_weight ='balanced')
model_accuracies(model = lr, x_feature=X_rus,y_label= y_rus, X_test=df_test.iloc[:788],df_test_len = 788)

"""## Following are the matrices for evaluating the model performance: Precision, Recall, F1-Score, AUC-ROC curve. Use F1-Score as the evaluation criteria for this project.

* Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.
* Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial

* F1 score for undersampled data for Model Logistic Regression and Nave bayes is more.

### * Try out models like Naive Bayes, Logistic Regression or SVM. Find out which one performs the best
"""

def run_parllel_job_cross_val(pipeline,x,y):
    big_future_x = client.scatter(x)
    big_future_y = client.scatter(y)
    a = client.submit(cross_val_score,pipeline, big_future_x, big_future_y, cv=5, scoring='f1', n_jobs=5)
    return a.result().mean()

pipe_lr = Pipeline(steps=[('scaler', StandardScaler()), ('lr', LogisticRegression())])
output = run_parllel_job_cross_val(pipe_lr,x,y)
output

pipe_svc = Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])
output = run_parllel_job_cross_val(pipe_svc,x,y)
output

pipe_nb = Pipeline(steps=[('nb', GaussianNB())])
output = run_parllel_job_cross_val(pipe_nb,x,y)
output

# scoring = 'f1'
# models = []
# results = []
# names = []
# models.append(('SVM', SVC()))
# models.append(('LR', LogisticRegression()))
# models.append(('LDA', LinearDiscriminantAnalysis()))
# models.append(('KNN', KNeighborsClassifier()))
# models.append(('CART', DecisionTreeClassifier()))
# models.append(('NB', GaussianNB()))

# for name, model in models:
#     start_time = time.time()
#     kfold = KFold(n_splits=10, random_state=42, shuffle=True)
#     cv_results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)
#     elapsed_time = time.time() - start_time
#     results.append(cv_results)
#     names.append(name)
#     msg = "{:3.2f} ({:3.2f})  Time elapsed: {:6.2f}".format(cv_results.mean(), cv_results.std(), elapsed_time)
#     msg = "%s "%(name) + msg
#     print(msg)

"""### * Ensemble Learning

#### RandomForest
"""

wrf = RandomForestClassifier(class_weight='balanced_subsample', random_state=42, n_jobs=5)
wrf.fit(x, y)

y_predict_wrf = wrf.predict(df_test)
cm_wrf = confusion_matrix(y_predict_wrf,y.sample(n = 56962))
sns.heatmap(cm_wrf,annot=True)
print("Accuracy Score :",accuracy_score(y_predict_wrf,y.sample(n = 56962)))

roc_curve_plots(y.sample(n = 56962),y_predict_wrf,df_test,wrf)

y_label = y.iloc[:df_test_len]

print("f1_score       : ", f1_score(y_label, y_predict_wrf, average="macro"))
print("precision_score: ", precision_score(y_label, y_predict_wrf, average="macro"))
print("recall_score   : ", recall_score(y_label, y_predict_wrf, average="macro"))

"""#### XGBoost
##### * Suppose, the dataset has 90 observations of negative class and 10 observations of positive class, then ideal value of scale_pos_weight should be 9.
"""

y.value_counts(normalize=True) * 100

xb = xgb.XGBClassifier(learning_rate=0.001, max_depth=1, n_estimators=1, scale_pos_weight=99.827, n_jobs=5)

xb.fit(x, y,eval_metric='logloss',verbose=True)

xgb_predict=xb.predict(df_test)
cm_xgb = confusion_matrix(xgb_predict,y.sample(n = 56962))
sns.heatmap(cm_xgb,annot=True)
print("Accuracy Score :",accuracy_score(xgb_predict,y.sample(n = 56962)))
print("f1_score       : ", f1_score(y.sample(n = 56962), xgb_predict, average="macro"))

"""#### BaggingClassifier"""

BC_clf = BaggingClassifier(base_estimator=SVC(),
                        n_estimators=10, random_state=0, n_jobs=5).fit(x, y)
BC_pred = BC_clf.predict(df_test)
cm_bc = confusion_matrix(BC_pred,y.sample(n = 56962))
sns.heatmap(cm_bc,annot=True)
print("Accuracy Score :",accuracy_score(BC_pred,y.sample(n = 56962)))
print("f1_score       : ", f1_score(y.sample(n = 56962), BC_pred, average="macro"))

BC_RFC = BaggingClassifier(base_estimator=RandomForestClassifier(class_weight='balanced_subsample'),
                        n_estimators=10, random_state=0, n_jobs=5).fit(x, y)
BC_rfc_pred = BC_RFC.predict(df_test)
cm_BC_rf = confusion_matrix(BC_rfc_pred,y.sample(n = 56962))
sns.heatmap(cm_BC_rf,annot=True)
print("Accuracy Score :",accuracy_score(BC_rfc_pred,y.sample(n = 56962)))
print("f1_score       : ", f1_score(y.sample(n = 56962), BC_rfc_pred, average="macro"))

"""#### GradientBoostingClassifier"""

GBC = GradientBoostingClassifier(random_state=0, loss='deviance', learning_rate=0.01, criterion='mse')
GBC.fit(x, y)
BC_gbc_pred = GBC.predict(df_test)
cm_gbc = confusion_matrix(BC_gbc_pred,y.sample(n = 56962))
sns.heatmap(cm_bc,annot=True)
print("Accuracy Score :",accuracy_score(BC_gbc_pred,y.sample(n = 56962)))
print("f1_score       : ", f1_score(y.sample(n = 56962), BC_gbc_pred, average="macro"))

"""#### ExtraTreesClassifier"""

GBC = ExtraTreesClassifier(n_estimators=100, random_state=0, class_weight='balanced', n_jobs = 6)
GBC.fit(x, y)
BC_gbc_pred = GBC.predict(df_test)
cm_gbc = confusion_matrix(BC_gbc_pred,y.sample(n = 56962))
sns.heatmap(cm_bc,annot=True)
print("Accuracy Score :",accuracy_score(BC_gbc_pred,y.sample(n = 56962)))
print("f1_score       : ", f1_score(y.sample(n = 56962), BC_gbc_pred, average="macro"))

"""#### AdaBoostClassifier"""

GBC = AdaBoostClassifier(n_estimators=100, random_state=0)
GBC.fit(x, y)
BC_gbc_pred = GBC.predict(df_test)
cm_gbc = confusion_matrix(BC_gbc_pred,y.sample(n = 56962))
sns.heatmap(cm_bc,annot=True)
print("Accuracy Score :",accuracy_score(BC_gbc_pred,y.sample(n = 56962)))
print("f1_score       : ", f1_score(y.sample(n = 56962), BC_gbc_pred, average="macro"))

# pipe = Pipeline([('scaler', StandardScaler()),('classifier', RandomForestClassifier())])
# estimators_range  =range(5,20)
# # Create space of candidate learning algorithms and their hyperparameters
# search_space = [{
#                      'scaler':[StandardScaler(),MinMaxScaler(), Normalizer(), MaxAbsScaler()],
#                      'classifier': [LogisticRegression()],
#                      'classifier__penalty' : ['l2'],'classifier__dual':[False], 'classifier__solver':['newton-cg', 'saga'], 'classifier__n_jobs':[5],
#                      'classifier__C': np.logspace(0, 4, 10)
#                 },
#                 {
#                      'scaler':[StandardScaler(),MinMaxScaler(), Normalizer(), MaxAbsScaler()],
#                      'classifier': [RandomForestClassifier()],
#                      'classifier__n_estimators': [10,100,200],'classifier__n_jobs':[5],'classifier__criterion':['gini', 'entropy'],
#                      'classifier__max_features': [1, 2, 3]
#                 },
#                 {
#                      'scaler':[StandardScaler(),MinMaxScaler(), Normalizer(), MaxAbsScaler()],
#                      'classifier': [xgb.XGBRFRegressor()],
#                      'classifier__learning_rate ': [0.1],
#                      'classifier__max_depth' : [5], 'classifier__n_estimators' : estimators_range, 'classifier__booster':['gbtree'], 'classifier__n_jobs':[5], 'classifier__verbosity' : [0]
#                 },
#                 {
#                     'scaler':[StandardScaler(),MinMaxScaler(), Normalizer(), MaxAbsScaler()],
#                     'classifier': [GaussianNB()],
#                 },
#                ]

# clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)
# best_model = clf.fit(x, y)

# print("best_score_ :",best_model.best_score_,"\n")
# best_model.best_estimator_.get_params()

"""### ANN"""

new_df.head()

sns.pairplot(new_df, diag_kind="kde")

new_df.shape

y_label = new_df.pop('Class')
x_feat = new_df

y_label.head()

x_feat.head()

x_train, x_test, y_train, y_test = train_test_split(x_feat, y_label, test_size=0.30, random_state=42, shuffle=True)

x_train.shape,x_test.shape,y_train.shape,y_test.shape

def create_model(optimizers=None,learn_rate=0.01, dropout_rate=0.2):
    model = Sequential()
    model.add(Dense(units =256,activation='relu',input_shape=(30,)))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(Dense(units=256,kernel_initializer='normal',activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(units=128,kernel_initializer='normal',activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(units=64,kernel_initializer='normal',activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(units=32,kernel_initializer='normal',activation='relu'))
    model.add(Dense(units=16,kernel_initializer='normal',activation='relu'))
    model.add(Dense(units=1,activation='sigmoid'))
    if not optimizers:
        optimizers = tf.keras.optimizers.Adam(lr=learn_rate)
    model.compile(optimizer=optimizers, loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.AUC()])
    return model

create_model().summary()

adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
create_model().compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.AUC()])

# print(os.getcwd())
logdir="logs/fit/"+ datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)
history = create_model().fit(x_train,y_train,epochs=20,batch_size=45,validation_data=(x_test, y_test))

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='lower right')
plt.show()

# %load_ext tensorboard
# %tensorboard --logdir {logs_base_dir}
# %reload_ext tensorboard
# model = model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.AUC()])

sklearn_model = KerasClassifier(build_fn=create_model, epochs=30)
batch_size = [10, 20, 40, 60, 80, 100]
epochs = [10, 50, 100]
learn_rate = [0.001, 0.02, 0.2]
dropout_rate = [0.0, 0.2, 0.4]
param_grid = dict(batch_size=batch_size, epochs=epochs,learn_rate=learn_rate, dropout_rate=dropout_rate)

grid = GridSearchCV(estimator=sklearn_model, param_grid=param_grid, n_jobs=2, cv=3)
grid_result = grid.fit(x_train,y_train,validation_data=(x_test, y_test))
print("\nBest: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("\n%f (%f) with: %r" % (mean, stdev, param))

optimizer_list = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
# sklearn_model = KerasClassifier(build_fn=create_model(), epochs=30)
param_grid = dict(optimizer=optimizer_list)
grid = GridSearchCV(estimator=sklearn_model, param_grid=param_grid, n_jobs=2, cv=3)
grid_result = grid.fit(x_train, y_train,validation_data=(x_test, y_test))
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

